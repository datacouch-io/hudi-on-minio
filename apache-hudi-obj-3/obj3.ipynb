{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports loaded \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import pyspark\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from faker import Faker\n",
    "import datetime\n",
    "from datetime import datetime\n",
    "import random\n",
    "import pandas as pd  # Import Pandas library for pretty printing\n",
    "\n",
    "print(\"Imports loaded \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUDI_VERSION = '0.14.0'\n",
    "SPARK_VERSION = '3.4'\n",
    "\n",
    "SUBMIT_ARGS = f\"--packages org.apache.hudi:hudi-spark{SPARK_VERSION}-bundle_2.12:{HUDI_VERSION},org.apache.hadoop:hadoop-aws:3.3.2 pyspark-shell\"\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = SUBMIT_ARGS\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .config('spark.serializer', 'org.apache.spark.serializer.KryoSerializer') \\\n",
    "    .config('spark.sql.extensions', 'org.apache.spark.sql.hudi.HoodieSparkSessionExtension') \\\n",
    "    .config('className', 'org.apache.hudi') \\\n",
    "    .config('spark.sql.hive.convertMetastoreParquet', 'false') \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.endpoint\", \"http://localhost:9000/\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.access.key\", \"admin\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.secret.key\", \"password\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.path.style.access\", \"true\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.connection.ssl.enabled\", \"false\")\n",
    "spark._jsc.hadoopConfiguration().set(\n",
    "    \"fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\")\n",
    "spark._jsc.hadoopConfiguration().set(\"fs.s3a.aws.credentials.provider\",\n",
    "                                     \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = \"default\"\n",
    "InventoryPath = f\"s3a://global-emart/hudi/database={db_name}/table_name=Inventory\"\n",
    "Order_ItemPath = f\"s3a://global-emart/hudi/database={db_name}/table_name=Order_Item\"\n",
    "OrderPath = f\"s3a://global-emart/hudi/database={db_name}/table_name=Order\"\n",
    "PaymentPath = f\"s3a://global-emart/hudi/database={db_name}/table_name=Payment\"\n",
    "ProductPath = f\"s3a://global-emart/hudi/database={db_name}/table_name=Product\"\n",
    "ShipmentPath = f\"s3a://global-emart/hudi/database={db_name}/table_name=Shipment\"\n",
    "UserPath = f\"s3a://global-emart/hudi/database={db_name}/table_name=User\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPSERT\n",
    "Scenario: Updating the price of Wireless Headphones.\n",
    "\n",
    "Description: Sarah Johnson updates the price of her listed product, Wireless Headphones, from $150.00 to $140.00. The existing record in the product table is updated with the new price.\n",
    "\n",
    "Operation: Upsert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- listing_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "existing_schema_df = spark.read.format(\"hudi\").load(ProductPath)\n",
    "existing_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- listing_date: string (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "s3a://huditest/hudi/database=default/table_name=Product\n",
      "\n",
      "\n",
      "error An error occurred while calling o307.save.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 105.0 failed 1 times, most recent failure: Lost task 0.0 in stage 105.0 (TID 442) (host.docker.internal executor driver): org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:342)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$mapPartitionsAsRDD$a3ab3c4$1(BaseSparkCommitActionExecutor.java:257)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:908)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:908)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1548)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1458)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1522)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.hudi.exception.HoodieRemoteException: Failed to create marker file /0ec1b41f-7437-4c3c-ad61-69c0404066a8-0_0-105-442_20240531113302101.parquet.marker.MERGE\n",
      "Connect to host.docker.internal:58979 [host.docker.internal/192.168.1.66] failed: Connection refused: no further information\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.executeCreateMarkerRequest(TimelineServerBasedWriteMarkers.java:186)\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.create(TimelineServerBasedWriteMarkers.java:141)\n",
      "\tat org.apache.hudi.table.marker.WriteMarkers.create(WriteMarkers.java:95)\n",
      "\tat org.apache.hudi.io.HoodieWriteHandle.createMarkerFile(HoodieWriteHandle.java:144)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandle.init(HoodieMergeHandle.java:198)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:134)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:125)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandleFactory.create(HoodieMergeHandleFactory.java:68)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.getUpdateHandle(BaseSparkCommitActionExecutor.java:400)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:368)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:335)\n",
      "\t... 29 more\n",
      "Caused by: org.apache.hudi.org.apache.http.conn.HttpHostConnectException: Connect to host.docker.internal:58979 [host.docker.internal/192.168.1.66] failed: Connection refused: no further information\n",
      "\tat org.apache.hudi.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:151)\n",
      "\tat org.apache.hudi.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n",
      "\tat org.apache.hudi.org.apache.http.client.fluent.Request.execute(Request.java:151)\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.executeRequestToTimelineServer(TimelineServerBasedWriteMarkers.java:232)\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.executeCreateMarkerRequest(TimelineServerBasedWriteMarkers.java:182)\n",
      "\t... 39 more\n",
      "Caused by: java.net.ConnectException: Connection refused: no further information\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.apache.hudi.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:74)\n",
      "\tat org.apache.hudi.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:134)\n",
      "\t... 52 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2790)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2726)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2725)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2725)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1211)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1211)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1211)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2989)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2928)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2917)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:976)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2263)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2284)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2303)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n",
      "\tat org.apache.spark.rdd.RDD.count(RDD.scala:1269)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.commitAndPerformPostOperations(HoodieSparkSqlWriter.scala:1050)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.writeInternal(HoodieSparkSqlWriter.scala:441)\n",
      "\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:132)\n",
      "\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:150)\n",
      "\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\n",
      "\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:842)\n",
      "Caused by: org.apache.hudi.exception.HoodieUpsertException: Error upserting bucketType UPDATE for partition :0\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:342)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.lambda$mapPartitionsAsRDD$a3ab3c4$1(BaseSparkCommitActionExecutor.java:257)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.api.java.JavaRDDLike.$anonfun$mapPartitionsWithIndex$1$adapted(JavaRDDLike.scala:102)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2(RDD.scala:908)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsWithIndex$2$adapted(RDD.scala:908)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$getOrCompute$1(RDD.scala:380)\n",
      "\tat org.apache.spark.storage.BlockManager.$anonfun$doPutIterator$1(BlockManager.scala:1548)\n",
      "\tat org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$doPut(BlockManager.scala:1458)\n",
      "\tat org.apache.spark.storage.BlockManager.doPutIterator(BlockManager.scala:1522)\n",
      "\tat org.apache.spark.storage.BlockManager.getOrElseUpdate(BlockManager.scala:1349)\n",
      "\tat org.apache.spark.rdd.RDD.getOrCompute(RDD.scala:378)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:329)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:92)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:161)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:139)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:554)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1529)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:557)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\t... 1 more\n",
      "Caused by: org.apache.hudi.exception.HoodieRemoteException: Failed to create marker file /0ec1b41f-7437-4c3c-ad61-69c0404066a8-0_0-105-442_20240531113302101.parquet.marker.MERGE\n",
      "Connect to host.docker.internal:58979 [host.docker.internal/192.168.1.66] failed: Connection refused: no further information\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.executeCreateMarkerRequest(TimelineServerBasedWriteMarkers.java:186)\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.create(TimelineServerBasedWriteMarkers.java:141)\n",
      "\tat org.apache.hudi.table.marker.WriteMarkers.create(WriteMarkers.java:95)\n",
      "\tat org.apache.hudi.io.HoodieWriteHandle.createMarkerFile(HoodieWriteHandle.java:144)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandle.init(HoodieMergeHandle.java:198)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:134)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandle.<init>(HoodieMergeHandle.java:125)\n",
      "\tat org.apache.hudi.io.HoodieMergeHandleFactory.create(HoodieMergeHandleFactory.java:68)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.getUpdateHandle(BaseSparkCommitActionExecutor.java:400)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpdate(BaseSparkCommitActionExecutor.java:368)\n",
      "\tat org.apache.hudi.table.action.commit.BaseSparkCommitActionExecutor.handleUpsertPartition(BaseSparkCommitActionExecutor.java:335)\n",
      "\t... 29 more\n",
      "Caused by: org.apache.hudi.org.apache.http.conn.HttpHostConnectException: Connect to host.docker.internal:58979 [host.docker.internal/192.168.1.66] failed: Connection refused: no further information\n",
      "\tat org.apache.hudi.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:151)\n",
      "\tat org.apache.hudi.org.apache.http.impl.conn.PoolingHttpClientConnectionManager.connect(PoolingHttpClientConnectionManager.java:353)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.MainClientExec.establishRoute(MainClientExec.java:380)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.MainClientExec.execute(MainClientExec.java:236)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.ProtocolExec.execute(ProtocolExec.java:184)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.RetryExec.execute(RetryExec.java:88)\n",
      "\tat org.apache.hudi.org.apache.http.impl.execchain.RedirectExec.execute(RedirectExec.java:110)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.InternalHttpClient.doExecute(InternalHttpClient.java:184)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:82)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:107)\n",
      "\tat org.apache.hudi.org.apache.http.impl.client.CloseableHttpClient.execute(CloseableHttpClient.java:55)\n",
      "\tat org.apache.hudi.org.apache.http.client.fluent.Request.execute(Request.java:151)\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.executeRequestToTimelineServer(TimelineServerBasedWriteMarkers.java:232)\n",
      "\tat org.apache.hudi.table.marker.TimelineServerBasedWriteMarkers.executeCreateMarkerRequest(TimelineServerBasedWriteMarkers.java:182)\n",
      "\t... 39 more\n",
      "Caused by: java.net.ConnectException: Connection refused: no further information\n",
      "\tat java.base/sun.nio.ch.Net.pollConnect(Native Method)\n",
      "\tat java.base/sun.nio.ch.Net.pollConnectNow(Net.java:672)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.timedFinishConnect(NioSocketImpl.java:554)\n",
      "\tat java.base/sun.nio.ch.NioSocketImpl.connect(NioSocketImpl.java:602)\n",
      "\tat java.base/java.net.SocksSocketImpl.connect(SocksSocketImpl.java:327)\n",
      "\tat java.base/java.net.Socket.connect(Socket.java:633)\n",
      "\tat org.apache.hudi.org.apache.http.conn.socket.PlainConnectionSocketFactory.connectSocket(PlainConnectionSocketFactory.java:74)\n",
      "\tat org.apache.hudi.org.apache.http.impl.conn.DefaultHttpClientConnectionOperator.connect(DefaultHttpClientConnectionOperator.java:134)\n",
      "\t... 52 more\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data for the updated product\n",
    "data = [\n",
    "    ('1', 'Wireless Headphones', 'Noise-cancelling wireless headphones', 'Electronics', '140.00', '1003', '2024-05-28')\n",
    "]\n",
    "\n",
    "columns = [\"product_id\", \"name\", \"description\", \"category\", \"price\", \"seller_id\", \"listing_date\"]\n",
    "product_df = spark.createDataFrame(data, columns)\n",
    "product_df.printSchema()\n",
    "def upsert_to_hudi(spark_df,\n",
    "                  table_name,\n",
    "                  db_name,\n",
    "                  method='upsert',\n",
    "                  table_type='COPY_ON_WRITE'\n",
    "                  ):\n",
    "    path = f\"s3a://huditest/hudi/database={db_name}/table_name={table_name}\"\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': table_name,\n",
    "        'hoodie.datasource.write.table.type': table_type,\n",
    "        'hoodie.datasource.write.table.name': table_name,\n",
    "        'hoodie.datasource.write.operation': method,\n",
    "        'hoodie.datasource.write.precombine.field': 'product_id',\n",
    "        'hoodie.datasource.write.recordkey.field': 'product_id',\n",
    "\n",
    "        \"hoodie.datasource.hive_sync.database\": db_name,\n",
    "        \"hoodie.datasource.hive_sync.table\": table_name,\n",
    "        \"hoodie.datasource.hive_sync.metastore.uris\": \"thrift://localhost:9083\",\n",
    "        \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "        \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    }\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(path)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    spark_df.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"append\"). \\\n",
    "        save(path)\n",
    "\n",
    "try:\n",
    "    upsert_to_hudi(\n",
    "        spark_df=product_df,\n",
    "        db_name=\"default\",\n",
    "        table_name=\"Product\"\n",
    "    )\n",
    "    print(\"Upserted\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"error\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+----------+-------------+--------------------+---------+-----+---------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|product_id|         name|         description| category|price|seller_id|       listing_date|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+----------+-------------+--------------------+---------+-----+---------+-------------------+\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         1|        Child|Turn last into re...|    sport|  638|     2536|2023-08-09 18:59:42|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         2|        Cover|As identify per d...|      job|  865|     2792|2023-03-07 20:04:31|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         3|     Economic|Church morning wi...|     west|  819|      289|2023-03-29 06:31:44|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         4|International|Meeting cover ass...|   window|  786|      416|2023-01-17 23:02:57|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         5|      Science|Treatment window ...|   expert|  131|     2842|2022-12-23 16:33:38|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         6|          All|Reveal study at m...|    think|  928|     4836|2022-12-14 21:14:33|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         7|        These|Never throughout ...|religious|  700|     3604|2023-03-19 06:44:49|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         8|      Kitchen|Gas director anim...|    happy|  577|     2316|2023-05-12 08:59:25|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|         9|       Doctor|There food develo...|     born|  972|     1159|2023-03-14 03:39:02|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        10|      Account|Various edge agre...|      now|   86|     1965|2023-08-07 17:05:38|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        11|          Arm|Report risk large...|    staff|  944|     1896|2022-12-01 23:28:43|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        12|       Writer|Per artist pull a...|   minute|  394|      346|2022-11-10 23:01:50|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        13|      Morning|What actually age...|    eight|   19|     2664|2022-10-17 10:59:39|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        14|        Wrong|Over main reflect...|      but|  469|     3254|2023-09-27 09:19:19|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        15|    Available|Tonight standard ...|     many|  744|     1049|2023-04-03 14:34:06|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        16|       Though|Environmental com...|    total|  448|     3027|2022-10-24 00:52:56|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        17|           Me|Listen else hear ...|     when|  813|     4676|2023-02-27 22:45:40|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        18|           On|Left ready school...|  project|  143|     4535|2023-02-09 14:03:33|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        19|            I|Impact rate nothi...|    white|  826|     4716|2023-02-08 00:09:07|\n",
      "|  20240531111427447|20240531111427447...|20240531111427447...|                      |0ec1b41f-7437-4c3...|        20|       Writer|Sport brother you...|      bad|  912|     2598|2022-10-31 11:08:35|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+----------+-------------+--------------------+---------+-----+---------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ProductsDF = spark.read.format(\"hudi\").load(ProductPath)\n",
    "# tripsDF.createOrReplaceTempView(\"trips_table\")\n",
    "ProductsDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert\n",
    "\n",
    "Scenario: Adding a new user, Sarah Johnson.\n",
    "\n",
    "Description: A new user, Sarah Johnson, registers on Global EMart. A new record is added to the user table with her details.\n",
    "\n",
    "Operation: Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- registration_date: string (nullable = true)\n",
      " |-- last_login_date: string (nullable = true)\n",
      "\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------+---------------+--------------------+--------------------+-----------------+-------------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|user_id|           name|               email|             country|registration_date|    last_login_date|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------+---------------+--------------------+--------------------+-----------------+-------------------+\n",
      "|  20240603164649566|20240603164649566...|20240603164649566...|                      |9d79cafa-f350-4cc...|      1|   Emily Howard|jaredbooth@exampl...|Cocos (Keeling) I...|       1970-03-26|2023-06-27 10:34:23|\n",
      "|  20240603164649566|20240603164649566...|20240603164649566...|                      |9d79cafa-f350-4cc...|      2|   James Conner|christopherburns@...|             Ireland|       1999-03-24|2023-07-05 00:29:18|\n",
      "|  20240603164649566|20240603164649566...|20240603164649566...|                      |9d79cafa-f350-4cc...|      3|   Derrick Wang|alexandra25@examp...|             Namibia|       1977-05-20|2023-09-13 12:56:13|\n",
      "|  20240603164649566|20240603164649566...|20240603164649566...|                      |9d79cafa-f350-4cc...|      4| Samantha Curry|  csimon@example.org|        Turkmenistan|       1996-09-26|2022-12-05 22:04:32|\n",
      "|  20240603164649566|20240603164649566...|20240603164649566...|                      |9d79cafa-f350-4cc...|      5|Jeffrey Sherman|   lhull@example.com|             Lesotho|       1969-01-02|2022-11-07 01:45:41|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------+---------------+--------------------+--------------------+-----------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(_hoodie_commit_time='20240603164649566', _hoodie_commit_seqno='20240603164649566_0_4996', _hoodie_record_key='20240603164649566_0_4996', _hoodie_partition_path='', _hoodie_file_name='9d79cafa-f350-4cc4-830c-9413fbc3b1f7-0_0-4-49_20240603171423045.parquet', user_id='4997', name='Steven Young', email='scott07@example.net', country='Guam', registration_date='1966-04-03', last_login_date='2023-02-24 12:48:16'),\n",
       " Row(_hoodie_commit_time='20240603164649566', _hoodie_commit_seqno='20240603164649566_0_4997', _hoodie_record_key='20240603164649566_0_4997', _hoodie_partition_path='', _hoodie_file_name='9d79cafa-f350-4cc4-830c-9413fbc3b1f7-0_0-4-49_20240603171423045.parquet', user_id='4998', name='Isaiah Cook', email='jennifercastro@example.net', country='Sierra Leone', registration_date='1944-12-20', last_login_date='2022-11-13 06:04:05'),\n",
       " Row(_hoodie_commit_time='20240603164649566', _hoodie_commit_seqno='20240603164649566_0_4998', _hoodie_record_key='20240603164649566_0_4998', _hoodie_partition_path='', _hoodie_file_name='9d79cafa-f350-4cc4-830c-9413fbc3b1f7-0_0-4-49_20240603171423045.parquet', user_id='4999', name='Patricia Price', email='robinsonjoseph@example.net', country='New Zealand', registration_date='1937-12-27', last_login_date='2023-06-11 05:35:07'),\n",
       " Row(_hoodie_commit_time='20240603164649566', _hoodie_commit_seqno='20240603164649566_0_4999', _hoodie_record_key='20240603164649566_0_4999', _hoodie_partition_path='', _hoodie_file_name='9d79cafa-f350-4cc4-830c-9413fbc3b1f7-0_0-4-49_20240603171423045.parquet', user_id='5000', name='Jessica Murphy', email='susan84@example.com', country='Paraguay', registration_date='1913-08-06', last_login_date='2023-06-20 03:47:12'),\n",
       " Row(_hoodie_commit_time='20240603171423045', _hoodie_commit_seqno='20240603171423045_0_5000', _hoodie_record_key='20240603171423045_15_0', _hoodie_partition_path='', _hoodie_file_name='9d79cafa-f350-4cc4-830c-9413fbc3b1f7-0_0-4-49_20240603171423045.parquet', user_id='5001', name='Sarah Johnson', email='sarah.johnson@example.com', country='USA', registration_date='2024-05-28', last_login_date='2024-05-28')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "existing_schema_df = spark.read.format(\"hudi\").load(UserPath)\n",
    "existing_schema_df.printSchema()\n",
    "existing_schema_df.show(5)\n",
    "existing_schema_df.tail(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      " |-- country: string (nullable = true)\n",
      " |-- registration_date: string (nullable = true)\n",
      " |-- last_login_date: string (nullable = true)\n",
      "\n",
      "\n",
      "\n",
      "s3a://global-emart/hudi/database=default/table_name=User\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data for the new user\n",
    "data = [\n",
    "    ('3', 'Sarah Johnson 2', 'sarah.johnson2@example.com', 'USA', '2024-05-28', '2024-05-28')\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"name\", \"email\", \"country\", \"registration_date\", \"last_login_date\"]\n",
    "user_df = spark.createDataFrame(data, columns)\n",
    "user_df.printSchema()\n",
    "\n",
    "def insert_to_hudi(spark_df,\n",
    "                  table_name,\n",
    "                  db_name,\n",
    "                  method='insert',\n",
    "                  table_type='COPY_ON_WRITE'\n",
    "                  ):\n",
    "    path = f\"s3a://global-emart/hudi/database={db_name}/table_name={table_name}\"\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': table_name,\n",
    "        'hoodie.datasource.write.table.type': table_type,\n",
    "        'hoodie.datasource.write.table.name': table_name,\n",
    "        'hoodie.datasource.write.operation': method,\n",
    "\n",
    "        \"hoodie.datasource.hive_sync.database\": db_name,\n",
    "        \"hoodie.datasource.hive_sync.table\": table_name,\n",
    "        \"hoodie.datasource.hive_sync.metastore.uris\": \"thrift://localhost:9083\",\n",
    "        \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "        \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    }\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(path)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    spark_df.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"append\"). \\\n",
    "        save(path)\n",
    "\n",
    "insert_to_hudi(\n",
    "    spark_df=user_df,\n",
    "    db_name=\"default\",\n",
    "    table_name=\"User\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BULK_INSERT\n",
    "\n",
    "Scenario: Bulk insert of a large number of new orders during a sale event.\n",
    "\n",
    "Description: During a major sales event, a large number of new orders are placed. These orders are inserted in bulk into the order table to efficiently handle the high volume of new data.\n",
    "\n",
    "Operation: Bulk Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_time: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "existing_schema_df = spark.read.format(\"hudi\").load(OrderPath)\n",
    "existing_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_time: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "existing_df = spark.read.format(\"hudi\").load(\"s3a://huditest/hudi/database=default/table_name=Order\")\n",
    "existing_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_time: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, DateType, DoubleType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"order_id\", StringType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"order_time\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"total_amount\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame with the explicit schema\n",
    "order_data = [\n",
    "    ('5001', '2024-05-29', '11:00:00', '1002', 200.00),\n",
    "    ('5002', '2024-05-29', '11:05:00', '1003', 350.00),\n",
    "    # Add more orders as needed\n",
    "]\n",
    "order_df = spark.createDataFrame(order_data, schema)\n",
    "order_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "s3a://global-emart/hudi/database=default/table_name=Order\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o132.save.\n: org.apache.hudi.exception.HoodieException: Config conflict(key\tcurrent value\texisting value):\nRecordKey:\torder_id\tnull\r\n\tat org.apache.hudi.HoodieWriterUtils$.validateTableConfig(HoodieWriterUtils.scala:211)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.writeInternal(HoodieSparkSqlWriter.scala:177)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:132)\r\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 32\u001b[0m\n\u001b[0;32m     26\u001b[0m     spark_df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhudi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m \\\n\u001b[0;32m     27\u001b[0m         options(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhudi_options)\u001b[38;5;241m.\u001b[39m \\\n\u001b[0;32m     28\u001b[0m         mode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mappend\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39m \\\n\u001b[0;32m     29\u001b[0m         save(path)\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Call the bulk_insert_to_hudi function with the correct DataFrame\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mbulk_insert_to_hudi\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdefault\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOrder\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m     36\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[13], line 29\u001b[0m, in \u001b[0;36mbulk_insert_to_hudi\u001b[1;34m(spark_df, table_name, db_name, method, table_type)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(path)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m \u001b[43mspark_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhudi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhudi_options\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m---> 29\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o132.save.\n: org.apache.hudi.exception.HoodieException: Config conflict(key\tcurrent value\texisting value):\nRecordKey:\torder_id\tnull\r\n\tat org.apache.hudi.HoodieWriterUtils$.validateTableConfig(HoodieWriterUtils.scala:211)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.writeInternal(HoodieSparkSqlWriter.scala:177)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:132)\r\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "def bulk_insert_to_hudi(spark_df, table_name, db_name, method='bulk_insert', table_type='COPY_ON_WRITE'):\n",
    "    path = f\"s3a://global-emart/hudi/database={db_name}/table_name={table_name}\"\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': table_name,\n",
    "        'hoodie.datasource.write.table.type': table_type,\n",
    "        'hoodie.datasource.write.operation': method,\n",
    "        'hoodie.datasource.write.recordkey.field': 'order_id',\n",
    "        'hoodie.datasource.write.precombine.field': 'order_date',\n",
    "        'hoodie.datasource.write.partitionpath.field': '',  # Add partition field if needed\n",
    "\n",
    "        'hoodie.datasource.hive_sync.database': db_name,\n",
    "        'hoodie.datasource.hive_sync.table': table_name,\n",
    "        'hoodie.datasource.hive_sync.metastore.uris': \"thrift://localhost:9083\",\n",
    "        'hoodie.datasource.hive_sync.mode': \"hms\",\n",
    "        'hoodie.datasource.hive_sync.enable': \"true\",\n",
    "        \n",
    "        'hoodie.datasource.write.schema.evolution.enable': 'true',\n",
    "        'hoodie.datasource.write.schema.evolution.validate': 'true'\n",
    "    }\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(path)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    spark_df.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"append\"). \\\n",
    "        save(path)\n",
    "\n",
    "# Call the bulk_insert_to_hudi function with the correct DataFrame\n",
    "bulk_insert_to_hudi(\n",
    "    spark_df=order_df,\n",
    "    db_name=\"default\",\n",
    "    table_name=\"Order\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE\n",
    "\n",
    "Scenario: Bob cancels his order for the Wireless Headphones.\n",
    "\n",
    "Description: Bob decides to cancel his order for the Wireless Headphones. The corresponding record is deleted from the order table.\n",
    "\n",
    "Operation: Delete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o162.save.\n: org.apache.hudi.exception.HoodieException: Config conflict(key\tcurrent value\texisting value):\nRecordKey:\torder_id\tnull\r\n\tat org.apache.hudi.HoodieWriterUtils$.validateTableConfig(HoodieWriterUtils.scala:211)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.writeInternal(HoodieSparkSqlWriter.scala:177)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:132)\r\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 18\u001b[0m\n\u001b[0;32m     10\u001b[0m hudi_options \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhoodie.table.name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOrder\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhoodie.datasource.write.recordkey.field\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_id\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhoodie.datasource.write.precombine.field\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124morder_date\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhoodie.datasource.write.operation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m }\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Write to Hudi table\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[43morder_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhudi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhudi_options\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mappend\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOrderPath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\readwriter.py:1463\u001b[0m, in \u001b[0;36mDataFrameWriter.save\u001b[1;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[0;32m   1461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1463\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o162.save.\n: org.apache.hudi.exception.HoodieException: Config conflict(key\tcurrent value\texisting value):\nRecordKey:\torder_id\tnull\r\n\tat org.apache.hudi.HoodieWriterUtils$.validateTableConfig(HoodieWriterUtils.scala:211)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.writeInternal(HoodieSparkSqlWriter.scala:177)\r\n\tat org.apache.hudi.HoodieSparkSqlWriter$.write(HoodieSparkSqlWriter.scala:132)\r\n\tat org.apache.hudi.DefaultSource.createRelation(DefaultSource.scala:150)\r\n\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:47)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:75)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:73)\r\n\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.executeCollect(commands.scala:84)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:118)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:195)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:103)\r\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:827)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:65)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\r\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:104)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\r\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\r\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:94)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:79)\r\n\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:133)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:856)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:387)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:360)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:239)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:842)\r\n"
     ]
    }
   ],
   "source": [
    "# Data for the canceled order (only record keys needed)\n",
    "data = [\n",
    "    ('1','2023-08-19','20:35:04','1210','3565.53')\n",
    "]\n",
    "\n",
    "columns = [\"order_id\", \"order_date\", \"order_time\", \"customer_id\", \"total_amount\"]\n",
    "order_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Hudi configuration\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'Order',\n",
    "    'hoodie.datasource.write.recordkey.field': 'order_id',\n",
    "    'hoodie.datasource.write.precombine.field': 'order_date',\n",
    "    'hoodie.datasource.write.operation': 'delete',\n",
    "}\n",
    "\n",
    "# Write to Hudi table\n",
    "order_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(OrderPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _hoodie_commit_time: string (nullable = true)\n",
      " |-- _hoodie_commit_seqno: string (nullable = true)\n",
      " |-- _hoodie_record_key: string (nullable = true)\n",
      " |-- _hoodie_partition_path: string (nullable = true)\n",
      " |-- _hoodie_file_name: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_date: string (nullable = true)\n",
      " |-- order_time: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      "\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+----------+----------+-----------+------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|order_id|order_date|order_time|customer_id|total_amount|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+----------+----------+-----------+------------+\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       1|2023-08-19|  20:35:04|       1210|     3565.53|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       2|2022-11-09|  04:28:59|         30|     1528.17|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       3|2020-06-07|  06:44:57|       1046|     2255.62|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       4|2022-03-11|  00:23:10|        488|      737.33|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       5|2023-05-30|  07:08:16|       3766|     4430.61|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       6|2022-12-01|  11:52:30|       3625|     2266.22|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       7|2021-04-05|  19:19:36|        498|     1634.57|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       8|2021-01-23|  23:23:34|        380|     4041.31|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|       9|2022-10-25|  12:01:19|       2726|     4826.66|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      10|2020-03-30|  00:34:25|       4905|     3956.61|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      11|2021-03-15|  20:24:16|       4348|      1202.0|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      12|2021-05-13|  09:20:30|        768|     2342.17|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      13|2023-03-26|  10:07:18|       3713|     1139.41|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      14|2021-06-27|  10:22:25|       1358|     4656.54|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      15|2023-01-06|  07:51:26|        756|     2891.37|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      16|2021-09-21|  00:38:55|       2685|      365.64|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      17|2023-09-20|  20:27:01|        125|     3909.08|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      18|2023-02-03|  15:01:21|       4126|     1344.18|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      19|2021-04-18|  12:38:44|       4382|     2522.24|\n",
      "|  20240603164629005|20240603164629005...|20240603164629005...|                      |915a6f56-a38a-43b...|      20|2021-09-27|  22:22:33|        189|     1379.11|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+--------+----------+----------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "existing_df = spark.read.format(\"hudi\").load(OrderPath)\n",
    "existing_df.printSchema()\n",
    "existing_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT_OVERWRITE\n",
    "\n",
    "Scenario: Overwriting existing user data with new data for the same users.\n",
    "\n",
    "Description: Existing user data for Alice Smith and Bob Brown needs to be updated with new details. The operation overwrites the existing records with the new data.\n",
    "\n",
    "Operation: Insert Overwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "s3a://global-emart/hudi/database=default/table_name=User\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New data for existing users\n",
    "data = [\n",
    "    ('1', 'Emily Howard', 'jaredbooth@example.net', 'USA', '2023-01-10', '2024-05-28'),\n",
    "    # ('2', 'James Conner', 'christopherburns@example.net', 'USA', '2023-02-15', '2024-05-28')\n",
    "]\n",
    "\n",
    "columns = [\"user_id\", \"name\", \"email\", \"country\", \"registration_date\", \"last_login_date\"]\n",
    "user_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Hudi configuration\n",
    "def insert_overwrite_to_hudi(spark_df,\n",
    "                  table_name,\n",
    "                  db_name,\n",
    "                  method='insert_overwrite',\n",
    "                  table_type='COPY_ON_WRITE'\n",
    "                  ):\n",
    "    path = f\"s3a://global-emart/hudi/database={db_name}/table_name={table_name}\"\n",
    "\n",
    "    hudi_options = {\n",
    "        'hoodie.table.name': table_name,\n",
    "        'hoodie.datasource.write.table.type': table_type,\n",
    "        'hoodie.datasource.write.table.name': table_name,\n",
    "        'hoodie.datasource.write.operation': method,\n",
    "\n",
    "        \"hoodie.datasource.hive_sync.database\": db_name,\n",
    "        \"hoodie.datasource.hive_sync.table\": table_name,\n",
    "        \"hoodie.datasource.hive_sync.metastore.uris\": \"thrift://localhost:9083\",\n",
    "        \"hoodie.datasource.hive_sync.mode\": \"hms\",\n",
    "        \"hoodie.datasource.hive_sync.enable\": \"true\",\n",
    "    }\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(path)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    spark_df.write.format(\"hudi\"). \\\n",
    "        options(**hudi_options). \\\n",
    "        mode(\"overwrite\"). \\\n",
    "        save(path)\n",
    "\n",
    "insert_overwrite_to_hudi(\n",
    "    spark_df=user_df,\n",
    "    db_name=\"default\",\n",
    "    table_name=\"User\"\n",
    ")\n",
    "\n",
    "# Write to Hudi table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------+------------+--------------------+-------+-----------------+---------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|  _hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|user_id|        name|               email|country|registration_date|last_login_date|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------+------------+--------------------+-------+-----------------+---------------+\n",
      "|  20240603173841641|20240603173841641...|20240603173841641...|                      |11093755-58ef-42a...|      1|Emily Howard|jaredbooth@exampl...|    USA|       2023-01-10|     2024-05-28|\n",
      "+-------------------+--------------------+--------------------+----------------------+--------------------+-------+------------+--------------------+-------+-----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "existing_df = spark.read.format(\"hudi\").load(UserPath)\n",
    "# existing_df.printSchema()\n",
    "existing_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INSERT_OVERWRITE_TABLE\n",
    "\n",
    "Scenario: Overwriting the entire product table with new product listings.\n",
    "\n",
    "Description: The entire product catalog is refreshed with new product listings. This operation overwrites all existing records in the product table with the new data.\n",
    "\n",
    "Operation: Insert Overwrite Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New data for products\n",
    "data = [\n",
    "    ('1', 'Laptop', 'High-performance laptop', 'Electronics', '1200.00', '1001', '2024-05-28'),\n",
    "    ('2', 'Smartphone', 'Latest model smartphone', 'Electronics', '800.00', '1002', '2024-05-28'),\n",
    "    # Add more products as needed\n",
    "]\n",
    "\n",
    "columns = [\"product_id\", \"name\", \"description\", \"category\", \"price\", \"seller_id\", \"listing_date\"]\n",
    "product_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Hudi configuration\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'product_table',\n",
    "    'hoodie.datasource.write.recordkey.field': 'product_id',\n",
    "    'hoodie.datasource.write.precombine.field': 'listing_date',\n",
    "    'hoodie.datasource.write.operation': 'insert_overwrite_table',\n",
    "}\n",
    "\n",
    "# Write to Hudi table\n",
    "product_df.write.format(\"hudi\").options(**hudi_options).mode(\"overwrite\").save(ProductPath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------------+------------------+----------------------+--------------------+----------+----------+--------------------+-----------+-------+---------+------------+\n",
      "|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|product_id|      name|         description|   category|  price|seller_id|listing_date|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+----------+----------+--------------------+-----------+-------+---------+------------+\n",
      "|  20240530203535673|20240530203535673...|                 1|                      |78c81db0-772c-42c...|         1|    Laptop|High-performance ...|Electronics|1200.00|     1001|  2024-05-28|\n",
      "|  20240530203535673|20240530203535673...|                 2|                      |78c81db0-772c-42c...|         2|Smartphone|Latest model smar...|Electronics| 800.00|     1002|  2024-05-28|\n",
      "+-------------------+--------------------+------------------+----------------------+--------------------+----------+----------+--------------------+-----------+-------+---------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "productDF = spark.read.format(\"hudi\").load(ProductPath)\n",
    "productDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DELETE_PARTITION\n",
    "\n",
    "Scenario: Removing all records for users from the USA.\n",
    "\n",
    "Description: All user records from the USA need to be removed from the user table. This operation deletes the entire partition corresponding to the country 'USA'.\n",
    "\n",
    "Operation: Delete Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkTypeError",
     "evalue": "[CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkTypeError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 7\u001b[0m\n\u001b[0;32m      2\u001b[0m data \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      3\u001b[0m     (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIreland\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m ]\n\u001b[0;32m      6\u001b[0m columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m----> 7\u001b[0m partition_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Hudi configuration\u001b[39;00m\n\u001b[0;32m     10\u001b[0m hudi_options \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhoodie.table.name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muser_table\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhoodie.datasource.write.operation\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelete_partition\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhoodie.datasource.write.partitionpath.field\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m }\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\session.py:1443\u001b[0m, in \u001b[0;36mSparkSession.createDataFrame\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1438\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_pandas \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, pd\u001b[38;5;241m.\u001b[39mDataFrame):\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;66;03m# Create a DataFrame from pandas DataFrame.\u001b[39;00m\n\u001b[0;32m   1440\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SparkSession, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mcreateDataFrame(  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m   1441\u001b[0m         data, schema, samplingRatio, verifySchema\n\u001b[0;32m   1442\u001b[0m     )\n\u001b[1;32m-> 1443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msamplingRatio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverifySchema\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m   1445\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\session.py:1485\u001b[0m, in \u001b[0;36mSparkSession._create_dataframe\u001b[1;34m(self, data, schema, samplingRatio, verifySchema)\u001b[0m\n\u001b[0;32m   1483\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_createFromRDD(data\u001b[38;5;241m.\u001b[39mmap(prepare), schema, samplingRatio)\n\u001b[0;32m   1484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1485\u001b[0m     rdd, struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_createFromLocal\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprepare\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1486\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1487\u001b[0m jrdd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mSerDeUtil\u001b[38;5;241m.\u001b[39mtoJavaArray(rdd\u001b[38;5;241m.\u001b[39m_to_java_object_rdd())\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\session.py:1093\u001b[0m, in \u001b[0;36mSparkSession._createFromLocal\u001b[1;34m(self, data, schema)\u001b[0m\n\u001b[0;32m   1090\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[0;32m   1092\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m schema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m-> 1093\u001b[0m     struct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inferSchemaFromList\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1094\u001b[0m     converter \u001b[38;5;241m=\u001b[39m _create_converter(struct)\n\u001b[0;32m   1095\u001b[0m     tupled_data: Iterable[Tuple] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(converter, data)\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\session.py:955\u001b[0m, in \u001b[0;36mSparkSession._inferSchemaFromList\u001b[1;34m(self, data, names)\u001b[0m\n\u001b[0;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[0;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[1;32m--> 955\u001b[0m schema \u001b[38;5;241m=\u001b[39m \u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    956\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_merge_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    957\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    958\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[0;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    972\u001b[0m     )\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\session.py:958\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    953\u001b[0m infer_array_from_first_element \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf\u001b[38;5;241m.\u001b[39mlegacyInferArrayTypeFromFirstElement()\n\u001b[0;32m    954\u001b[0m prefer_timestamp_ntz \u001b[38;5;241m=\u001b[39m is_timestamp_ntz_preferred()\n\u001b[0;32m    955\u001b[0m schema \u001b[38;5;241m=\u001b[39m reduce(\n\u001b[0;32m    956\u001b[0m     _merge_type,\n\u001b[0;32m    957\u001b[0m     (\n\u001b[1;32m--> 958\u001b[0m         \u001b[43m_infer_schema\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    959\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    960\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    961\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_dict_as_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    962\u001b[0m \u001b[43m            \u001b[49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minfer_array_from_first_element\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    963\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefer_timestamp_ntz\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    964\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m data\n\u001b[0;32m    966\u001b[0m     ),\n\u001b[0;32m    967\u001b[0m )\n\u001b[0;32m    968\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _has_nulltype(schema):\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[0;32m    970\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_DETERMINE_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    971\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{},\n\u001b[0;32m    972\u001b[0m     )\n",
      "File \u001b[1;32mc:\\python\\lib\\site-packages\\pyspark\\sql\\types.py:1670\u001b[0m, in \u001b[0;36m_infer_schema\u001b[1;34m(row, names, infer_dict_as_struct, infer_array_from_first_element, prefer_timestamp_ntz)\u001b[0m\n\u001b[0;32m   1667\u001b[0m     items \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(row\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mitems())\n\u001b[0;32m   1669\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1670\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   1671\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCANNOT_INFER_SCHEMA_FOR_TYPE\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1672\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(row)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[0;32m   1673\u001b[0m     )\n\u001b[0;32m   1675\u001b[0m fields \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   1676\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m items:\n",
      "\u001b[1;31mPySparkTypeError\u001b[0m: [CANNOT_INFER_SCHEMA_FOR_TYPE] Can not infer schema for type: `str`."
     ]
    }
   ],
   "source": [
    "# Data for the partition to delete\n",
    "data = [\n",
    "    ('Ireland')\n",
    "]\n",
    "\n",
    "columns = [\"country\"]\n",
    "partition_df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Hudi configuration\n",
    "hudi_options = {\n",
    "    'hoodie.table.name': 'user_table',\n",
    "    'hoodie.datasource.write.operation': 'delete_partition',\n",
    "    'hoodie.datasource.write.partitionpath.field': 'country',\n",
    "}\n",
    "\n",
    "# Write to Hudi table\n",
    "partition_df.write.format(\"hudi\").options(**hudi_options).mode(\"append\").save(UserPath)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
